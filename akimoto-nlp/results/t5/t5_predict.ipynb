{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9nr2pMoMyXX",
        "outputId": "dd3a179b-dbf1-4eeb-aeda-8ea7c8f1ce37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.4.2 sentencepiece"
      ],
      "metadata": {
        "id": "CRRK6ddYPXJ9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer"
      ],
      "metadata": {
        "id": "2GkZbnUcPRKa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = '/content/drive/MyDrive/Colab Notebooks/akimoto/nlp'\n",
        "\n",
        "# Models (SentencePiece)\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n"
      ],
      "metadata": {
        "id": "9a4BaU1GWg6b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "# 各種ハイパーパラメータ\n",
        "args_dict = dict(\n",
        "    max_input_length=64,\n",
        "    max_target_length=512,\n",
        "    train_batch_size=8,\n",
        "    eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        ")\n",
        "\n",
        "args = argparse.Namespace(**args_dict)\n"
      ],
      "metadata": {
        "id": "B-PGN7dONQVK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    return s\n"
      ],
      "metadata": {
        "id": "RsZkzrRPQGNe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    assert \"\\n\" not in text and \"\\r\" not in text\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.strip()\n",
        "    text = normalize_neologd(text)\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "-oHYyDiYP4jV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "MAX_SOURCE_LENGTH = args.max_input_length   # 入力される記事本文の最大トークン数\n",
        "MAX_TARGET_LENGTH = args.max_target_length  # 生成されるタイトルの最大トークン数\n",
        "\n",
        "def preprocess_title(text):\n",
        "    return normalize_text(text.replace(\"\\n\", \"\"))\n",
        "\n",
        "# 推論モード設定\n",
        "trained_model.eval()\n",
        "\n",
        "def generate_texts(title):\n",
        "    # 前処理とトークナイズを行う\n",
        "    inputs = [preprocess_title(title)]\n",
        "    batch = tokenizer.batch_encode_plus(\n",
        "        inputs, max_length=MAX_SOURCE_LENGTH, truncation=True, \n",
        "        padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = batch['input_ids']\n",
        "    input_mask = batch['attention_mask']\n",
        "\n",
        "    USE_GPU = False\n",
        "    if USE_GPU:\n",
        "        input_ids = input_ids.cuda()\n",
        "        input_mask = input_mask.cuda()\n",
        "    # 生成処理を行う\n",
        "    outputs = trained_model.generate(\n",
        "        input_ids=input_ids, attention_mask=input_mask, \n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        # temperature=1.0,  # 生成にランダム性を入れる温度パラメータ\n",
        "        # num_beams=10,  # ビームサーチの探索幅\n",
        "        # diversity_penalty=1.0,  # 生成結果の多様性を生み出すためのペナルティパラメータ\n",
        "        # num_beam_groups=10,  # ビームサーチのグループ\n",
        "        # num_return_sequences=10,  # 生成する文の数\n",
        "        repetition_penalty=8.0,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
        "    )\n",
        "\n",
        "    # 生成されたトークン列を文字列に変換する\n",
        "    generated_bodies = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                                        clean_up_tokenization_spaces=False) \n",
        "                        for ids in outputs]\n",
        "\n",
        "    # 生成された文章を表示する\n",
        "    for i, body in enumerate(generated_bodies):\n",
        "        print(\"\\n\".join(textwrap.wrap(f\"{i+1:2}. {body}\")))\n"
      ],
      "metadata": {
        "id": "MJ-FTyPENGAa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_texts('I SEE...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17-v31b8nFsa",
        "outputId": "bb37d161-573e-4901-e244-aeb9d06330ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1764: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. eah i'm sorry...i'm sorry...愛はどこへ消えたのだろう?何度泣いても涙が止まらなくて悲しくて泣きたくなるよ\n",
            "君に会いたいって思えるまでずっとそばにいたかっただけど、どうすればいいのか分からなかった。なぜだかわからないけど誰かと会ったことなんてないん\n",
            "だから僕たちは何も言わないでしょ?誰にも言わず黙って見守ってくれるはずなのに誰も気づいてくれない恋なんかするわけないじゃん!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_texts('根も葉もRUMOR')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_4Ar-2AnJxd",
        "outputId": "bb39241a-9a46-4839-cd7c-27d3f8ab5896"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1764: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  next_indices = next_tokens // vocab_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. ey!花は咲いてるのに何だか寂しくなるよ君と出会ってからもう1年経つんだね僕たちはいつの間にか仲良くなっちゃった恋なんかしたくなくて\n",
            "さっきまで声を掛けてたけど今さら聞けるわけないじゃん根も葉もrumorみんなでワイワイ楽しく過ごした日々が嘘のように過ぎ去って行くその瞬間(\n",
            "とき)をどう生きるのか?生き方なんて人それぞれだけど愛とは何か?それとも誰かと一緒にいたいどんな風に吹かれながら歩き出してみようかなあそびた\n",
            "りして...\n"
          ]
        }
      ]
    }
  ]
}