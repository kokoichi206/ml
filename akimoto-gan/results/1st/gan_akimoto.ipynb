{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan-akimoto.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY8BtSTNO79h",
        "outputId": "db285b6e-187a-4fb7-f3db-c84425e0e34d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XMCbpn3SwaSa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383efe0b-edd6-444f-b1e5-3242be05955a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 0s (20.3 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "git-lfs/2.3.4 (GitHub; linux amd64; go 1.8.3)\n",
            "Initialized empty Git repository in /content/.git/\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'ml'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 117 (delta 39), reused 107 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (117/117), 16.40 MiB | 22.24 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "Filtering content: 100% (3/3), 44.51 MiB | 37.54 MiB/s, done.\n",
            "total 45640\n",
            "drwxr-xr-x 4 root root     4096 Jan  2 17:02 .\n",
            "drwxr-xr-x 5 root root     4096 Jan  2 17:02 ..\n",
            "-rw-r--r-- 1 root root 12055448 Jan  2 17:02 akb.zip\n",
            "-rw-r--r-- 1 root root     2443 Jan  2 17:02 get_names.py\n",
            "-rw-r--r-- 1 root root       42 Jan  2 17:02 .gitattributes\n",
            "-rw-r--r-- 1 root root       26 Jan  2 17:02 .gitignore\n",
            "-rw-r--r-- 1 root root     1530 Jan  2 17:02 google_img_search.py\n",
            "-rw-r--r-- 1 root root      733 Jan  2 17:02 memo.md\n",
            "-rw-r--r-- 1 root root      801 Jan  2 17:02 README.md\n",
            "-rw-r--r-- 1 root root      744 Jan  2 17:02 resize_for_folder.py\n",
            "-rw-r--r-- 1 root root     1245 Jan  2 17:02 resize.py\n",
            "drwxr-xr-x 6 root root     4096 Jan  2 17:02 results\n",
            "-rw-r--r-- 1 root root 22387781 Jan  2 17:02 saiko.zip\n",
            "-rw-r--r-- 1 root root 12230145 Jan  2 17:02 saka.zip\n",
            "-rw-r--r-- 1 root root      513 Jan  2 17:02 save_imgs.py\n",
            "drwxr-xr-x 2 root root     4096 Jan  2 17:02 scripts\n",
            "-rw-r--r-- 1 root root     1436 Jan  2 17:02 train.py\n",
            "akb.zip  drive\tml  saka.zip  sample_data\n",
            "   3847    3847   42325\n",
            "   3928    3928   43216\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install git-lfs\n",
        "!git lfs version\n",
        "!rm -rf ml\n",
        "!mkdir ml\n",
        "!cd ml\n",
        "!git init\n",
        "!git lfs install\n",
        "!git clone https://github.com/kokoichi206/ml.git\n",
        "!git fetch\n",
        "!cd ..\n",
        "!ls -la ml/akimoto/\n",
        "!mv ml/akimoto/saka.zip saka.zip\n",
        "!mv ml/akimoto/akb.zip akb.zip\n",
        "\n",
        "!rm -rf akb_pre\n",
        "!rm -rf saka_pre\n",
        "\n",
        "!ls\n",
        "!yes | unzip akb.zip > /dev/null\n",
        "!yes | unzip saka.zip > /dev/null\n",
        "\n",
        "!ls akb_pre | wc\n",
        "!ls saka_pre | wc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "34IOGKkSAeos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=\"saka_pre\", label_mode=None, image_size=(64, 64), batch_size=32,\n",
        "    shuffle=True\n",
        ").map(lambda x: x/255.0)"
      ],
      "metadata": {
        "id": "KQTZuabswngQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b3c86aa-a28b-4631-9eda-21d745c7d3dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3928 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64,64,3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9c_amRlAjLy",
        "outputId": "11267e3c-573a-4672-ddbe-363283ab426c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 128)         262272    \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 404,801\n",
            "Trainable params: 404,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=(latent_dim, )),\n",
        "        layers.Dense(8*8*128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),  # 16*16\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"), # 32*32\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"), # 64*64\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "generator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwhkFXmoBMOL",
        "outputId": "078e752b-6cce-469c-8711-d3d6b0ab9d8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 8192)              1056768   \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 16, 16, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2DT  (None, 32, 32, 256)      524544    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_5 (Conv2DT  (None, 64, 64, 512)      2097664   \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 64, 64, 3)         38403     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,979,651\n",
            "Trainable params: 3,979,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_gen = keras.optimizers.Adam(1e-4)\n",
        "opt_disc = keras.optimizers.Adam(1e-4)\n",
        "loss_fn = keras.losses.BinaryCrossentropy()"
      ],
      "metadata": {
        "id": "VVPj_yDECG9u"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(f\"======= i: {i} =======\")\n",
        "    for epoch in range(100*i, 100*(i+1)):\n",
        "        for idx, (real) in enumerate(tqdm(dataset)):\n",
        "            batch_size = real.shape[0]\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                random_latent_vectors = tf.random.normal(shape = (batch_size, latent_dim))\n",
        "                fake = generator(random_latent_vectors)\n",
        "\n",
        "            # 1epoch に１回写真を保存\n",
        "            if idx == 0:\n",
        "                img = keras.preprocessing.image.array_to_img(fake[0])\n",
        "                img.save(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_generated/epoch_%04d.png\" % (epoch))\n",
        "\n",
        "            ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "            with tf.GradientTape() as disc_tape:\n",
        "                loss_disc_real = loss_fn(tf.ones((batch_size, 1)), discriminator(real))\n",
        "                loss_disc_fake = loss_fn(tf.zeros((batch_size, 1)), discriminator(fake))\n",
        "                loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
        "\n",
        "            grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)\n",
        "            opt_disc.apply_gradients(\n",
        "                zip(grads, discriminator.trainable_weights)\n",
        "            )\n",
        "\n",
        "            ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "            with tf.GradientTape() as gen_tape:\n",
        "                fake = generator(random_latent_vectors)\n",
        "                output = discriminator(fake)\n",
        "                loss_gen = loss_fn(tf.ones(batch_size, 1), output)\n",
        "\n",
        "            grads = gen_tape.gradient(loss_gen, generator.trainable_weights)\n",
        "            opt_gen.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "\n",
        "    # 途中でcolabが止まった時の対策に、定期的にparameterを保存\n",
        "    gen_string = generator.to_json()\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/gen_model.json\", mode='w') as f:\n",
        "        f.write(gen_string)\n",
        "    generator.save_weights(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/gen_weights.hdf5\")\n",
        "\n",
        "    disc_string = discriminator.to_json()\n",
        "    with open(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/disc_model.json\", mode='w') as f:\n",
        "        f.write(disc_string)\n",
        "    discriminator.save_weights(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/disc_weights.hdf5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thBwBPRgCStW",
        "outputId": "0a6fa5ea-59a0-4c84-f528-1a9244efe42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.68it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.68it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.68it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.69it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.69it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.74it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.70it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.63it/s]\n",
            "100%|██████████| 123/123 [00:24<00:00,  5.01it/s]\n",
            "100%|██████████| 123/123 [00:23<00:00,  5.30it/s]\n",
            "100%|██████████| 123/123 [00:22<00:00,  5.46it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.69it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.73it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.76it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.74it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.75it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.74it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.73it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.76it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.74it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.76it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.76it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.73it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.73it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "100%|██████████| 123/123 [00:40<00:00,  3.00it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.71it/s]\n",
            "100%|██████████| 123/123 [00:21<00:00,  5.72it/s]\n",
            "  2%|▏         | 2/123 [00:00<00:24,  5.02it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_string = generator.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/gen_model.json\", mode='w') as f:\n",
        "    f.write(gen_string)\n",
        "generator.save_weights(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/gen_weights.hdf5\")\n",
        "\n",
        "disc_string = discriminator.to_json()\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/disc_model.json\", mode='w') as f:\n",
        "    f.write(disc_string)\n",
        "discriminator.save_weights(\"/content/drive/MyDrive/Colab Notebooks/akimoto/gan_models/disc_weights.hdf5\")\n"
      ],
      "metadata": {
        "id": "kfhr94uK81yr"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}